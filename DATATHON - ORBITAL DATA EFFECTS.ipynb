{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f584271",
   "metadata": {},
   "source": [
    "# RETRIEVING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecc339df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\kruti\\anaconda3\\lib\\site-packages (2.3.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d84e03d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     INTLDES  NORAD_CAT_ID  OBJECT_TYPE     SATNAME COUNTRY      LAUNCH  \\\n",
      "0  1957-001A             1  ROCKET BODY    SL-1 R/B     CIS  1957-10-04   \n",
      "1  1957-001B             2      PAYLOAD   SPUTNIK 1     CIS  1957-10-04   \n",
      "2  1957-002A             3      PAYLOAD   SPUTNIK 2     CIS  1957-11-03   \n",
      "3  1958-001A             4      PAYLOAD  EXPLORER 1      US  1958-02-01   \n",
      "4  1958-002B             5      PAYLOAD  VANGUARD 1      US  1958-03-17   \n",
      "\n",
      "    SITE       DECAY  PERIOD  INCLINATION  ...  RCSVALUE  RCS_SIZE  FILE  \\\n",
      "0  TTMTR  1957-12-01   96.19        65.10  ...         0     LARGE     1   \n",
      "1  TTMTR  1958-01-03   96.10        65.00  ...         0       NaN  7179   \n",
      "2  TTMTR  1958-04-14  103.74        65.33  ...         0     SMALL  9221   \n",
      "3  AFETR  1970-03-31   88.48        33.15  ...         0       NaN     1   \n",
      "4  AFETR         NaN  132.61        34.26  ...         0     SMALL  9242   \n",
      "\n",
      "   LAUNCH_YEAR  LAUNCH_NUM LAUNCH_PIECE  CURRENT  OBJECT_NAME  OBJECT_ID  \\\n",
      "0         1957           1            A        Y     SL-1 R/B  1957-001A   \n",
      "1         1957           1            B        Y    SPUTNIK 1  1957-001B   \n",
      "2         1957           2            A        Y    SPUTNIK 2  1957-002A   \n",
      "3         1958           1            A        Y   EXPLORER 1  1958-001A   \n",
      "4         1958           2            B        Y   VANGUARD 1  1958-002B   \n",
      "\n",
      "  OBJECT_NUMBER  \n",
      "0             1  \n",
      "1             2  \n",
      "2             3  \n",
      "3             4  \n",
      "4             5  \n",
      "\n",
      "[5 rows x 24 columns]\n",
      "65265 rows loaded\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Space-Track login\n",
    "USERNAME = \"krutiray95@gmail.com\"\n",
    "PASSWORD = \"Kxrwebster9439316894!\"\n",
    "\n",
    "session = requests.Session()\n",
    "login_url = \"https://www.space-track.org/ajaxauth/login\"\n",
    "login_payload = {\"identity\": USERNAME, \"password\": PASSWORD}\n",
    "session.post(login_url, data=login_payload)\n",
    "\n",
    "# Query SATCAT CSV\n",
    "url = \"https://www.space-track.org/basicspacedata/query/class/satcat/orderby/NORAD_CAT_ID asc/format/csv\"\n",
    "response = session.get(url)\n",
    "\n",
    "# Save CSV locally\n",
    "with open(\"satcat.csv\", \"wb\") as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "# Load into pandas\n",
    "df = pd.read_csv(\"satcat.csv\")\n",
    "print(df.head())\n",
    "print(len(df), \"rows loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f54d9358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   NORAD_CAT_ID  OBJECT_NUMBER     OBJECT_NAME     INTLDES   OBJECT_ID  RCS  \\\n",
      "0         44179          44179  MICROSAT-R DEB  2019-006BS  2019-006BS    0   \n",
      "1         44179          44179  MICROSAT-R DEB  2019-006BS  2019-006BS    0   \n",
      "2         26476          26476           DM-F3   2000-048A   2000-048A    0   \n",
      "3         26476          26476           DM-F3   2000-048A   2000-048A    0   \n",
      "4         44117          44117  MICROSAT-R DEB   2019-006C   2019-006C    0   \n",
      "\n",
      "  RCS_SIZE COUNTRY            MSG_EPOCH         DECAY_EPOCH     SOURCE  \\\n",
      "0    SMALL     IND  2020-12-24 21:50:16  2020-01-01 0:00:00     satcat   \n",
      "1    SMALL     IND  2020-01-06 20:44:00  2020-01-01 0:00:00  decay_msg   \n",
      "2    LARGE      US  2019-12-29 09:02:00  2020-01-01 0:59:00    TIP_msg   \n",
      "3    LARGE      US  2019-12-30 03:58:00  2020-01-01 4:08:00    TIP_msg   \n",
      "4    SMALL     IND  2019-11-21 19:40:04  2020-01-02 0:00:00  60day_msg   \n",
      "\n",
      "     MSG_TYPE  PRECEDENCE  \n",
      "0  Historical           1  \n",
      "1  Historical           2  \n",
      "2  Prediction           3  \n",
      "3  Prediction           3  \n",
      "4  Prediction           4  \n",
      "63694 rows loaded\n"
     ]
    }
   ],
   "source": [
    "import os, requests, pandas as pd\n",
    "\n",
    "USER = os.getenv(\"SPACETRACK_USER\", \"krutiray95@gmail.com\")\n",
    "PASS = os.getenv(\"SPACETRACK_PASS\", \"Kxrwebster9439316894!\")\n",
    "\n",
    "s = requests.Session()\n",
    "s.post(\"https://www.space-track.org/ajaxauth/login\",\n",
    "       data={\"identity\": USER, \"password\": PASS})\n",
    "\n",
    "url = (\"https://www.space-track.org/basicspacedata/query/\"\n",
    "       \"class/decay/DECAY_EPOCH/%3E2020-01-01/\"\n",
    "       \"orderby/DECAY_EPOCH asc/format/csv\")\n",
    "\n",
    "r = s.get(url); r.raise_for_status()\n",
    "open(\"decay.csv\",\"wb\").write(r.content)\n",
    "\n",
    "df = pd.read_csv(\"decay.csv\")\n",
    "print(df.head()); print(len(df), \"rows loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843433e0",
   "metadata": {},
   "source": [
    "# DATA CLEANING PROCESS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada81dff",
   "metadata": {},
   "source": [
    "## Imports & Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6384d014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SATCAT_FILE: .\\satcat.csv\n",
      "DECAY_FILE : .\\decay.csv\n",
      "TLE_FILE   : .\\st_krutiray95@gmail_com_20250831_1445368442.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---- Configure file paths here ----\n",
    "DATA_DIR = \".\"  # folder with your CSVs ('.' = current folder)\n",
    "\n",
    "SATCAT_FILE = os.path.join(DATA_DIR, \"satcat.csv\")\n",
    "DECAY_FILE  = os.path.join(DATA_DIR, \"decay.csv\")\n",
    "\n",
    "# If you know your TLE filename, set it directly:\n",
    "# TLE_FILE = os.path.join(DATA_DIR, \"my_tle_export.csv\")\n",
    "\n",
    "# Otherwise: auto-detect a Space-Track export that starts with 'st_' or fall back to any csv named 'gp*.csv'\n",
    "tlcands = [f for f in os.listdir(DATA_DIR) if f.lower().endswith(\".csv\") and (f.startswith(\"st_\") or f.lower().startswith(\"gp\"))]\n",
    "TLE_FILE = os.path.join(DATA_DIR, tlcands[0]) if tlcands else None\n",
    "\n",
    "print(\"SATCAT_FILE:\", SATCAT_FILE)\n",
    "print(\"DECAY_FILE :\", DECAY_FILE)\n",
    "print(\"TLE_FILE   :\", TLE_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0281f19",
   "metadata": {},
   "source": [
    "## Utility Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41e9b88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_cols(df):\n",
    "    df = df.copy()\n",
    "    df.columns = [c.strip().upper() for c in df.columns]\n",
    "    return df\n",
    "\n",
    "def parse_dt(series):\n",
    "    return pd.to_datetime(series, utc=True, errors=\"coerce\")\n",
    "\n",
    "def num(series):\n",
    "    return pd.to_numeric(series, errors=\"coerce\")\n",
    "\n",
    "def save(df, path):\n",
    "    df.to_csv(path, index=False)\n",
    "    print(f\"✔ Saved: {path}  ({len(df):,} rows)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c076d12",
   "metadata": {},
   "source": [
    "## Cleaning SATCAT (metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67ab6607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Saved: .\\satcat_clean.csv  (65,265 rows)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NORAD_CAT_ID</th>\n",
       "      <th>SATNAME</th>\n",
       "      <th>INTLDES</th>\n",
       "      <th>COUNTRY</th>\n",
       "      <th>LAUNCH</th>\n",
       "      <th>DECAY</th>\n",
       "      <th>PERIOD</th>\n",
       "      <th>APOGEE</th>\n",
       "      <th>PERIGEE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>SL-1 R/B</td>\n",
       "      <td>1957-001A</td>\n",
       "      <td>CIS</td>\n",
       "      <td>1957-10-04 00:00:00+00:00</td>\n",
       "      <td>1957-12-01 00:00:00+00:00</td>\n",
       "      <td>96.19</td>\n",
       "      <td>938.0</td>\n",
       "      <td>214.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>SPUTNIK 1</td>\n",
       "      <td>1957-001B</td>\n",
       "      <td>CIS</td>\n",
       "      <td>1957-10-04 00:00:00+00:00</td>\n",
       "      <td>1958-01-03 00:00:00+00:00</td>\n",
       "      <td>96.10</td>\n",
       "      <td>1080.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>SPUTNIK 2</td>\n",
       "      <td>1957-002A</td>\n",
       "      <td>CIS</td>\n",
       "      <td>1957-11-03 00:00:00+00:00</td>\n",
       "      <td>1958-04-14 00:00:00+00:00</td>\n",
       "      <td>103.74</td>\n",
       "      <td>1659.0</td>\n",
       "      <td>211.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>EXPLORER 1</td>\n",
       "      <td>1958-001A</td>\n",
       "      <td>US</td>\n",
       "      <td>1958-02-01 00:00:00+00:00</td>\n",
       "      <td>1970-03-31 00:00:00+00:00</td>\n",
       "      <td>88.48</td>\n",
       "      <td>215.0</td>\n",
       "      <td>183.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>VANGUARD 1</td>\n",
       "      <td>1958-002B</td>\n",
       "      <td>US</td>\n",
       "      <td>1958-03-17 00:00:00+00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>132.61</td>\n",
       "      <td>3823.0</td>\n",
       "      <td>648.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   NORAD_CAT_ID     SATNAME    INTLDES COUNTRY                    LAUNCH  \\\n",
       "0             1    SL-1 R/B  1957-001A     CIS 1957-10-04 00:00:00+00:00   \n",
       "1             2   SPUTNIK 1  1957-001B     CIS 1957-10-04 00:00:00+00:00   \n",
       "2             3   SPUTNIK 2  1957-002A     CIS 1957-11-03 00:00:00+00:00   \n",
       "3             4  EXPLORER 1  1958-001A      US 1958-02-01 00:00:00+00:00   \n",
       "4             5  VANGUARD 1  1958-002B      US 1958-03-17 00:00:00+00:00   \n",
       "\n",
       "                      DECAY  PERIOD  APOGEE  PERIGEE  \n",
       "0 1957-12-01 00:00:00+00:00   96.19   938.0    214.0  \n",
       "1 1958-01-03 00:00:00+00:00   96.10  1080.0     64.0  \n",
       "2 1958-04-14 00:00:00+00:00  103.74  1659.0    211.0  \n",
       "3 1970-03-31 00:00:00+00:00   88.48   215.0    183.0  \n",
       "4                       NaT  132.61  3823.0    648.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Null rates:\n",
      " DECAY           0.4792\n",
      "PERIOD          0.0152\n",
      "PERIGEE         0.0152\n",
      "APOGEE          0.0152\n",
      "NORAD_CAT_ID    0.0000\n",
      "LAUNCH          0.0000\n",
      "COUNTRY         0.0000\n",
      "INTLDES         0.0000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Load\n",
    "satcat = pd.read_csv(SATCAT_FILE, low_memory=False)\n",
    "satcat = clean_cols(satcat)\n",
    "\n",
    "# Keep typical useful columns if present\n",
    "keep_satcat = [\n",
    "    \"NORAD_CAT_ID\",\"SATNAME\",\"INTLDES\",\"TYPE\",\"COUNTRY\",\n",
    "    \"LAUNCH\",\"DECAY\",\"PERIOD\",\"INCL\",\"APOGEE\",\"PERIGEE\",\"RCS\"\n",
    "]\n",
    "satcat = satcat[[c for c in keep_satcat if c in satcat.columns]].copy()\n",
    "\n",
    "# Types\n",
    "if \"NORAD_CAT_ID\" in satcat: satcat[\"NORAD_CAT_ID\"] = num(satcat[\"NORAD_CAT_ID\"]).astype(\"Int64\")\n",
    "if \"LAUNCH\" in satcat: satcat[\"LAUNCH\"] = parse_dt(satcat[\"LAUNCH\"])\n",
    "if \"DECAY\"  in satcat: satcat[\"DECAY\"]  = parse_dt(satcat[\"DECAY\"])\n",
    "for c in [\"PERIOD\",\"INCL\",\"APOGEE\",\"PERIGEE\",\"RCS\"]:\n",
    "    if c in satcat.columns: satcat[c] = num(satcat[c])\n",
    "\n",
    "# Quality: drop null IDs and dedupe to one row per NORAD\n",
    "satcat = satcat.dropna(subset=[\"NORAD_CAT_ID\"]).drop_duplicates(\"NORAD_CAT_ID\")\n",
    "\n",
    "# Save\n",
    "SATCAT_CLEAN = os.path.join(DATA_DIR, \"satcat_clean.csv\")\n",
    "save(satcat, SATCAT_CLEAN)\n",
    "\n",
    "# Quick QA\n",
    "display(satcat.head())\n",
    "print(\"\\nNull rates:\\n\", satcat.isna().mean().sort_values(ascending=False).head(8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dbc92e",
   "metadata": {},
   "source": [
    "## Cleaning DECAY/REENTRY (Space-Track class/decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d15546d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Saved: .\\decay_clean.csv  (63,694 rows)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NORAD_CAT_ID</th>\n",
       "      <th>OBJECT_NAME</th>\n",
       "      <th>INTLDES</th>\n",
       "      <th>COUNTRY</th>\n",
       "      <th>RCS</th>\n",
       "      <th>MSG_EPOCH</th>\n",
       "      <th>DECAY_EPOCH</th>\n",
       "      <th>SOURCE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44179</td>\n",
       "      <td>MICROSAT-R DEB</td>\n",
       "      <td>2019-006BS</td>\n",
       "      <td>IND</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-12-24 21:50:16+00:00</td>\n",
       "      <td>2020-01-01 00:00:00+00:00</td>\n",
       "      <td>satcat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44179</td>\n",
       "      <td>MICROSAT-R DEB</td>\n",
       "      <td>2019-006BS</td>\n",
       "      <td>IND</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-01-06 20:44:00+00:00</td>\n",
       "      <td>2020-01-01 00:00:00+00:00</td>\n",
       "      <td>decay_msg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26476</td>\n",
       "      <td>DM-F3</td>\n",
       "      <td>2000-048A</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-12-29 09:02:00+00:00</td>\n",
       "      <td>2020-01-01 00:59:00+00:00</td>\n",
       "      <td>TIP_msg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26476</td>\n",
       "      <td>DM-F3</td>\n",
       "      <td>2000-048A</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-12-30 03:58:00+00:00</td>\n",
       "      <td>2020-01-01 04:08:00+00:00</td>\n",
       "      <td>TIP_msg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44117</td>\n",
       "      <td>MICROSAT-R DEB</td>\n",
       "      <td>2019-006C</td>\n",
       "      <td>IND</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-11-21 19:40:04+00:00</td>\n",
       "      <td>2020-01-02 00:00:00+00:00</td>\n",
       "      <td>60day_msg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   NORAD_CAT_ID     OBJECT_NAME     INTLDES COUNTRY  RCS  \\\n",
       "0         44179  MICROSAT-R DEB  2019-006BS     IND    0   \n",
       "1         44179  MICROSAT-R DEB  2019-006BS     IND    0   \n",
       "2         26476           DM-F3   2000-048A      US    0   \n",
       "3         26476           DM-F3   2000-048A      US    0   \n",
       "4         44117  MICROSAT-R DEB   2019-006C     IND    0   \n",
       "\n",
       "                  MSG_EPOCH               DECAY_EPOCH     SOURCE  \n",
       "0 2020-12-24 21:50:16+00:00 2020-01-01 00:00:00+00:00     satcat  \n",
       "1 2020-01-06 20:44:00+00:00 2020-01-01 00:00:00+00:00  decay_msg  \n",
       "2 2019-12-29 09:02:00+00:00 2020-01-01 00:59:00+00:00    TIP_msg  \n",
       "3 2019-12-30 03:58:00+00:00 2020-01-01 04:08:00+00:00    TIP_msg  \n",
       "4 2019-11-21 19:40:04+00:00 2020-01-02 00:00:00+00:00  60day_msg  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Date range (DECAY_EPOCH): 2020-01-01 00:00:00+00:00 → 2025-10-25 00:00:00+00:00\n"
     ]
    }
   ],
   "source": [
    "decay = pd.read_csv(DECAY_FILE, low_memory=False)\n",
    "decay = clean_cols(decay)\n",
    "\n",
    "# Common columns in 'decay' class:\n",
    "keep_decay = [\n",
    "    \"NORAD_CAT_ID\",\"OBJECT_NAME\",\"INTLDES\",\"COUNTRY\",\"RCS\",\n",
    "    \"MSG_EPOCH\",\"DECAY_EPOCH\",\"SOURCE\",\"TYPE\",\"ELSET\"\n",
    "]\n",
    "decay = decay[[c for c in keep_decay if c in decay.columns]].copy()\n",
    "\n",
    "# Types\n",
    "if \"NORAD_CAT_ID\" in decay: decay[\"NORAD_CAT_ID\"] = num(decay[\"NORAD_CAT_ID\"]).astype(\"Int64\")\n",
    "for c in [\"MSG_EPOCH\",\"DECAY_EPOCH\"]:\n",
    "    if c in decay.columns: decay[c] = parse_dt(decay[c])\n",
    "if \"RCS\" in decay.columns: decay[\"RCS\"] = num(decay[\"RCS\"])\n",
    "\n",
    "# Clean\n",
    "decay = decay.dropna(subset=[\"NORAD_CAT_ID\"]).drop_duplicates()\n",
    "\n",
    "# Save\n",
    "DECAY_CLEAN = os.path.join(DATA_DIR, \"decay_clean.csv\")\n",
    "save(decay, DECAY_CLEAN)\n",
    "\n",
    "# QA\n",
    "display(decay.head())\n",
    "print(\"\\nDate range (DECAY_EPOCH):\", decay.get(\"DECAY_EPOCH\", pd.Series(dtype=\"datetime64[ns, UTC]\")).min(),\n",
    "      \"→\", decay.get(\"DECAY_EPOCH\", pd.Series(dtype=\"datetime64[ns, UTC]\")).max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d399175",
   "metadata": {},
   "source": [
    "## Cleaning TLE/GP and derive features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cd5fe17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Saved: .\\gp_clean.csv  (9,571 rows)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NORAD_CAT_ID</th>\n",
       "      <th>EPOCH</th>\n",
       "      <th>MEAN_MOTION</th>\n",
       "      <th>ECCENTRICITY</th>\n",
       "      <th>BSTAR</th>\n",
       "      <th>INCLINATION</th>\n",
       "      <th>MEAN_ANOMALY</th>\n",
       "      <th>ARG_OF_PERICENTER</th>\n",
       "      <th>RA_OF_ASC_NODE</th>\n",
       "      <th>TLE_LINE1</th>\n",
       "      <th>TLE_LINE2</th>\n",
       "      <th>A_KM</th>\n",
       "      <th>HP_KM</th>\n",
       "      <th>HA_KM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25544</td>\n",
       "      <td>2020-01-01 06:11:10.983264+00:00</td>\n",
       "      <td>15.495303</td>\n",
       "      <td>0.000505</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>51.6446</td>\n",
       "      <td>3.7257</td>\n",
       "      <td>90.7711</td>\n",
       "      <td>99.4171</td>\n",
       "      <td>1 25544U 98067A   20001.25776601  .00001469  0...</td>\n",
       "      <td>2 25544  51.6446  99.4171 0005055  90.7711   3...</td>\n",
       "      <td>6796.236040</td>\n",
       "      <td>414.663542</td>\n",
       "      <td>421.534537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25544</td>\n",
       "      <td>2020-01-01 12:23:48.583392+00:00</td>\n",
       "      <td>15.495288</td>\n",
       "      <td>0.000514</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>51.6442</td>\n",
       "      <td>7.2280</td>\n",
       "      <td>91.7041</td>\n",
       "      <td>98.1370</td>\n",
       "      <td>1 25544U 98067A   20001.51653453  .00000882  0...</td>\n",
       "      <td>2 25544  51.6442  98.1370 0005144  91.7041   7...</td>\n",
       "      <td>6796.240557</td>\n",
       "      <td>414.607571</td>\n",
       "      <td>421.599543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25544</td>\n",
       "      <td>2020-01-01 14:05:26.578752+00:00</td>\n",
       "      <td>15.495300</td>\n",
       "      <td>0.000534</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>51.6440</td>\n",
       "      <td>40.6973</td>\n",
       "      <td>92.2059</td>\n",
       "      <td>97.7889</td>\n",
       "      <td>1 25544U 98067A   20001.58711318  .00001214  0...</td>\n",
       "      <td>2 25544  51.6440  97.7889 0005337  92.2059  40...</td>\n",
       "      <td>6796.236990</td>\n",
       "      <td>414.472838</td>\n",
       "      <td>421.727142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25544</td>\n",
       "      <td>2020-01-01 17:17:49.889184+00:00</td>\n",
       "      <td>15.495307</td>\n",
       "      <td>0.000534</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>51.6443</td>\n",
       "      <td>65.8338</td>\n",
       "      <td>92.8411</td>\n",
       "      <td>97.1279</td>\n",
       "      <td>1 25544U 98067A   20001.72071631  .00001295  0...</td>\n",
       "      <td>2 25544  51.6443  97.1279 0005339  92.8411  65...</td>\n",
       "      <td>6796.235092</td>\n",
       "      <td>414.469582</td>\n",
       "      <td>421.726602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25544</td>\n",
       "      <td>2020-01-01 18:47:02.666400+00:00</td>\n",
       "      <td>15.495305</td>\n",
       "      <td>0.000518</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>51.6444</td>\n",
       "      <td>51.8986</td>\n",
       "      <td>92.6004</td>\n",
       "      <td>96.8194</td>\n",
       "      <td>1 25544U 98067A   20001.78266975  .00001093  0...</td>\n",
       "      <td>2 25544  51.6444  96.8194 0005179  92.6004  51...</td>\n",
       "      <td>6796.235648</td>\n",
       "      <td>414.578877</td>\n",
       "      <td>421.618418</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   NORAD_CAT_ID                            EPOCH  MEAN_MOTION  ECCENTRICITY  \\\n",
       "0         25544 2020-01-01 06:11:10.983264+00:00    15.495303      0.000505   \n",
       "1         25544 2020-01-01 12:23:48.583392+00:00    15.495288      0.000514   \n",
       "2         25544 2020-01-01 14:05:26.578752+00:00    15.495300      0.000534   \n",
       "3         25544 2020-01-01 17:17:49.889184+00:00    15.495307      0.000534   \n",
       "4         25544 2020-01-01 18:47:02.666400+00:00    15.495305      0.000518   \n",
       "\n",
       "      BSTAR  INCLINATION  MEAN_ANOMALY  ARG_OF_PERICENTER  RA_OF_ASC_NODE  \\\n",
       "0  0.000034      51.6446        3.7257            90.7711         99.4171   \n",
       "1  0.000024      51.6442        7.2280            91.7041         98.1370   \n",
       "2  0.000030      51.6440       40.6973            92.2059         97.7889   \n",
       "3  0.000031      51.6443       65.8338            92.8411         97.1279   \n",
       "4  0.000028      51.6444       51.8986            92.6004         96.8194   \n",
       "\n",
       "                                           TLE_LINE1  \\\n",
       "0  1 25544U 98067A   20001.25776601  .00001469  0...   \n",
       "1  1 25544U 98067A   20001.51653453  .00000882  0...   \n",
       "2  1 25544U 98067A   20001.58711318  .00001214  0...   \n",
       "3  1 25544U 98067A   20001.72071631  .00001295  0...   \n",
       "4  1 25544U 98067A   20001.78266975  .00001093  0...   \n",
       "\n",
       "                                           TLE_LINE2         A_KM       HP_KM  \\\n",
       "0  2 25544  51.6446  99.4171 0005055  90.7711   3...  6796.236040  414.663542   \n",
       "1  2 25544  51.6442  98.1370 0005144  91.7041   7...  6796.240557  414.607571   \n",
       "2  2 25544  51.6440  97.7889 0005337  92.2059  40...  6796.236990  414.472838   \n",
       "3  2 25544  51.6443  97.1279 0005339  92.8411  65...  6796.235092  414.469582   \n",
       "4  2 25544  51.6444  96.8194 0005179  92.6004  51...  6796.235648  414.578877   \n",
       "\n",
       "        HA_KM  \n",
       "0  421.534537  \n",
       "1  421.599543  \n",
       "2  421.727142  \n",
       "3  421.726602  \n",
       "4  421.618418  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Counts per satellite (top 5):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NORAD_CAT_ID\n",
       "25544    9571\n",
       "Name: count, dtype: Int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if TLE_FILE is None:\n",
    "    raise FileNotFoundError(\"No TLE CSV found. Set TLE_FILE above or place a file (e.g. 'st_*.csv') next to the notebook.\")\n",
    "\n",
    "gp = pd.read_csv(TLE_FILE, low_memory=False)\n",
    "gp = clean_cols(gp)\n",
    "\n",
    "# Normalize common alternative column names if needed\n",
    "rename_map = {\n",
    "    \"MEANMOTION\":\"MEAN_MOTION\",\n",
    "    \"ECC\":\"ECCENTRICITY\",\n",
    "    \"BSTARDRAG\":\"BSTAR\",\n",
    "    \"INCL\":\"INCLINATION\",\n",
    "}\n",
    "for k,v in rename_map.items():\n",
    "    if k in gp.columns and v not in gp.columns:\n",
    "        gp = gp.rename(columns={k:v})\n",
    "\n",
    "# Expected core columns—filter to what's available\n",
    "expected_gp = [\n",
    "    \"NORAD_CAT_ID\",\"EPOCH\",\"MEAN_MOTION\",\"ECCENTRICITY\",\"BSTAR\",\"INCLINATION\",\n",
    "    \"MEAN_ANOMALY\",\"ARG_OF_PERICENTER\",\"RA_OF_ASC_NODE\",\"TLE_LINE1\",\"TLE_LINE2\"\n",
    "]\n",
    "gp = gp[[c for c in expected_gp if c in gp.columns]].copy()\n",
    "\n",
    "# Types\n",
    "if \"NORAD_CAT_ID\" in gp: gp[\"NORAD_CAT_ID\"] = num(gp[\"NORAD_CAT_ID\"]).astype(\"Int64\")\n",
    "if \"EPOCH\" in gp: gp[\"EPOCH\"] = parse_dt(gp[\"EPOCH\"])\n",
    "for c in [\"MEAN_MOTION\",\"ECCENTRICITY\",\"BSTAR\",\"INCLINATION\",\"MEAN_ANOMALY\",\"ARG_OF_PERICENTER\",\"RA_OF_ASC_NODE\"]:\n",
    "    if c in gp.columns: gp[c] = num(gp[c])\n",
    "\n",
    "# Drop bad rows, sort, de-dupe key (NORAD,EPOCH)\n",
    "gp = gp.dropna(subset=[\"NORAD_CAT_ID\",\"EPOCH\"]).sort_values([\"NORAD_CAT_ID\",\"EPOCH\"])\n",
    "gp = gp.drop_duplicates(subset=[\"NORAD_CAT_ID\",\"EPOCH\"], keep=\"last\").reset_index(drop=True)\n",
    "\n",
    "# Sanity bounds (adjust to your regime)\n",
    "if \"ECCENTRICITY\" in gp: gp = gp[(gp[\"ECCENTRICITY\"] >= 0) & (gp[\"ECCENTRICITY\"] < 1)]\n",
    "if \"MEAN_MOTION\"  in gp: gp = gp[(gp[\"MEAN_MOTION\"] > 0) & (gp[\"MEAN_MOTION\"] < 25)]\n",
    "\n",
    "# Derive semi-major axis & altitudes (km)\n",
    "MU = 398600.4418  # km^3/s^2\n",
    "RE_KM = 6378.137\n",
    "if {\"MEAN_MOTION\",\"ECCENTRICITY\"}.issubset(gp.columns):\n",
    "    n_rad_s = gp[\"MEAN_MOTION\"] * 2*np.pi / 86400.0\n",
    "    a_km = (MU / (n_rad_s**2)) ** (1/3)\n",
    "    e = gp[\"ECCENTRICITY\"].clip(lower=0, upper=0.999999)\n",
    "    rp_km = a_km * (1 - e)\n",
    "    ra_km = a_km * (1 + e)\n",
    "    gp[\"A_KM\"]  = a_km\n",
    "    gp[\"HP_KM\"] = rp_km - RE_KM  # perigee altitude\n",
    "    gp[\"HA_KM\"] = ra_km - RE_KM  # apogee altitude\n",
    "\n",
    "# Save cleaned GP\n",
    "GP_CLEAN = os.path.join(DATA_DIR, \"gp_clean.csv\")\n",
    "save(gp, GP_CLEAN)\n",
    "\n",
    "# QA\n",
    "display(gp.head())\n",
    "print(\"\\nCounts per satellite (top 5):\")\n",
    "display(gp[\"NORAD_CAT_ID\"].value_counts().head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407b0e65",
   "metadata": {},
   "source": [
    "## daily time series + simple anomaly flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4645614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Saved: .\\gp_daily_features.csv  (1,827 rows)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EPOCH</th>\n",
       "      <th>BSTAR</th>\n",
       "      <th>MEAN_MOTION</th>\n",
       "      <th>HP_KM</th>\n",
       "      <th>HA_KM</th>\n",
       "      <th>A_KM</th>\n",
       "      <th>NORAD_CAT_ID</th>\n",
       "      <th>MM_Z</th>\n",
       "      <th>BSTAR_Z</th>\n",
       "      <th>ANOMALY_FLAG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-01 00:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25544</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-02 00:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25544</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-03 00:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25544</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-04 00:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25544</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-05 00:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25544</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      EPOCH  BSTAR  MEAN_MOTION  HP_KM  HA_KM  A_KM  \\\n",
       "0 2020-01-01 00:00:00+00:00    NaN          NaN    NaN    NaN   NaN   \n",
       "1 2020-01-02 00:00:00+00:00    NaN          NaN    NaN    NaN   NaN   \n",
       "2 2020-01-03 00:00:00+00:00    NaN          NaN    NaN    NaN   NaN   \n",
       "3 2020-01-04 00:00:00+00:00    NaN          NaN    NaN    NaN   NaN   \n",
       "4 2020-01-05 00:00:00+00:00    NaN          NaN    NaN    NaN   NaN   \n",
       "\n",
       "   NORAD_CAT_ID  MM_Z  BSTAR_Z  ANOMALY_FLAG  \n",
       "0         25544   NaN      NaN         False  \n",
       "1         25544   NaN      NaN         False  \n",
       "2         25544   NaN      NaN         False  \n",
       "3         25544   NaN      NaN         False  \n",
       "4         25544   NaN      NaN         False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Daily rows: 1827\n"
     ]
    }
   ],
   "source": [
    "def build_daily(gp):\n",
    "    out = []\n",
    "    for norad, g in gp.groupby(\"NORAD_CAT_ID\"):\n",
    "        g = g.set_index(\"EPOCH\").sort_index()\n",
    "        g = g[~g.index.duplicated(keep=\"last\")]           # remove duplicate timestamps\n",
    "        cols = [c for c in [\"BSTAR\",\"MEAN_MOTION\",\"HP_KM\",\"HA_KM\",\"A_KM\"] if c in g.columns]\n",
    "        if not cols: \n",
    "            continue\n",
    "        daily = g[cols].resample(\"1D\").interpolate(\"time\") # daily interpolation\n",
    "        daily[\"NORAD_CAT_ID\"] = norad\n",
    "        out.append(daily.reset_index())\n",
    "    return pd.concat(out, ignore_index=True) if out else pd.DataFrame(columns=[\"EPOCH\",\"NORAD_CAT_ID\"])\n",
    "\n",
    "gp_daily = build_daily(gp)\n",
    "\n",
    "# Rolling 7-day median/STD z-scores (tune thresholds later)\n",
    "if \"MEAN_MOTION\" in gp_daily:\n",
    "    mm_med = gp_daily.groupby(\"NORAD_CAT_ID\")[\"MEAN_MOTION\"].transform(lambda s: s.rolling(7, min_periods=3).median())\n",
    "    mm_std = gp_daily.groupby(\"NORAD_CAT_ID\")[\"MEAN_MOTION\"].transform(lambda s: s.rolling(7, min_periods=3).std(ddof=0))\n",
    "    gp_daily[\"MM_Z\"] = (gp_daily[\"MEAN_MOTION\"] - mm_med) / (mm_std + 1e-9)\n",
    "\n",
    "if \"BSTAR\" in gp_daily:\n",
    "    b_med = gp_daily.groupby(\"NORAD_CAT_ID\")[\"BSTAR\"].transform(lambda s: s.rolling(7, min_periods=3).median())\n",
    "    b_std = gp_daily.groupby(\"NORAD_CAT_ID\")[\"BSTAR\"].transform(lambda s: s.rolling(7, min_periods=3).std(ddof=0))\n",
    "    gp_daily[\"BSTAR_Z\"] = (gp_daily[\"BSTAR\"] - b_med) / (b_std + 1e-9)\n",
    "\n",
    "if {\"MM_Z\",\"BSTAR_Z\"}.issubset(gp_daily.columns):\n",
    "    gp_daily[\"ANOMALY_FLAG\"] = (gp_daily[\"MM_Z\"] > 2.0) | (gp_daily[\"BSTAR_Z\"] > 2.0)\n",
    "\n",
    "GP_DAILY = os.path.join(DATA_DIR, \"gp_daily_features.csv\")\n",
    "save(gp_daily, GP_DAILY)\n",
    "\n",
    "display(gp_daily.head())\n",
    "print(\"\\nDaily rows:\", len(gp_daily))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01883c85",
   "metadata": {},
   "source": [
    "## Joining daily orbital features with SATCAT metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff1505b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Saved: .\\orbital_daily_with_meta.csv  (1,827 rows)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EPOCH</th>\n",
       "      <th>BSTAR</th>\n",
       "      <th>MEAN_MOTION</th>\n",
       "      <th>HP_KM</th>\n",
       "      <th>HA_KM</th>\n",
       "      <th>A_KM</th>\n",
       "      <th>NORAD_CAT_ID</th>\n",
       "      <th>MM_Z</th>\n",
       "      <th>BSTAR_Z</th>\n",
       "      <th>ANOMALY_FLAG</th>\n",
       "      <th>SATNAME</th>\n",
       "      <th>COUNTRY</th>\n",
       "      <th>LAUNCH</th>\n",
       "      <th>DECAY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-01 00:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25544</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>ISS (ZARYA)</td>\n",
       "      <td>ISS</td>\n",
       "      <td>1998-11-20 00:00:00+00:00</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-02 00:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25544</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>ISS (ZARYA)</td>\n",
       "      <td>ISS</td>\n",
       "      <td>1998-11-20 00:00:00+00:00</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-03 00:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25544</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>ISS (ZARYA)</td>\n",
       "      <td>ISS</td>\n",
       "      <td>1998-11-20 00:00:00+00:00</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-04 00:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25544</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>ISS (ZARYA)</td>\n",
       "      <td>ISS</td>\n",
       "      <td>1998-11-20 00:00:00+00:00</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-05 00:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25544</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>ISS (ZARYA)</td>\n",
       "      <td>ISS</td>\n",
       "      <td>1998-11-20 00:00:00+00:00</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      EPOCH  BSTAR  MEAN_MOTION  HP_KM  HA_KM  A_KM  \\\n",
       "0 2020-01-01 00:00:00+00:00    NaN          NaN    NaN    NaN   NaN   \n",
       "1 2020-01-02 00:00:00+00:00    NaN          NaN    NaN    NaN   NaN   \n",
       "2 2020-01-03 00:00:00+00:00    NaN          NaN    NaN    NaN   NaN   \n",
       "3 2020-01-04 00:00:00+00:00    NaN          NaN    NaN    NaN   NaN   \n",
       "4 2020-01-05 00:00:00+00:00    NaN          NaN    NaN    NaN   NaN   \n",
       "\n",
       "   NORAD_CAT_ID  MM_Z  BSTAR_Z  ANOMALY_FLAG      SATNAME COUNTRY  \\\n",
       "0         25544   NaN      NaN         False  ISS (ZARYA)     ISS   \n",
       "1         25544   NaN      NaN         False  ISS (ZARYA)     ISS   \n",
       "2         25544   NaN      NaN         False  ISS (ZARYA)     ISS   \n",
       "3         25544   NaN      NaN         False  ISS (ZARYA)     ISS   \n",
       "4         25544   NaN      NaN         False  ISS (ZARYA)     ISS   \n",
       "\n",
       "                     LAUNCH DECAY  \n",
       "0 1998-11-20 00:00:00+00:00   NaT  \n",
       "1 1998-11-20 00:00:00+00:00   NaT  \n",
       "2 1998-11-20 00:00:00+00:00   NaT  \n",
       "3 1998-11-20 00:00:00+00:00   NaT  \n",
       "4 1998-11-20 00:00:00+00:00   NaT  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "meta_cols = [c for c in [\"NORAD_CAT_ID\",\"SATNAME\",\"TYPE\",\"COUNTRY\",\"LAUNCH\",\"DECAY\"] if c in satcat.columns]\n",
    "joined = gp_daily.merge(satcat[meta_cols], on=\"NORAD_CAT_ID\", how=\"left\")\n",
    "\n",
    "JOINED_FILE = os.path.join(DATA_DIR, \"orbital_daily_with_meta.csv\")\n",
    "save(joined, JOINED_FILE)\n",
    "\n",
    "display(joined.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d31a1bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate (NORAD,EPOCH) rows in gp_clean: 0\n",
      "Ecc min/max: 3e-06 0.0013583\n",
      "MM min/max: 15.48249857 15.51817712\n",
      "\n",
      "Missingness (gp_clean):\n",
      " NORAD_CAT_ID         0.0\n",
      "EPOCH                0.0\n",
      "MEAN_MOTION          0.0\n",
      "ECCENTRICITY         0.0\n",
      "BSTAR                0.0\n",
      "INCLINATION          0.0\n",
      "MEAN_ANOMALY         0.0\n",
      "ARG_OF_PERICENTER    0.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Duplicates in key\n",
    "dups = gp.duplicated(subset=[\"NORAD_CAT_ID\",\"EPOCH\"]).sum()\n",
    "print(\"Duplicate (NORAD,EPOCH) rows in gp_clean:\", dups)\n",
    "\n",
    "# Ranges\n",
    "if \"ECCENTRICITY\" in gp: print(\"Ecc min/max:\", gp[\"ECCENTRICITY\"].min(), gp[\"ECCENTRICITY\"].max())\n",
    "if \"MEAN_MOTION\" in gp:  print(\"MM min/max:\", gp[\"MEAN_MOTION\"].min(), gp[\"MEAN_MOTION\"].max())\n",
    "\n",
    "# Missingness snapshot\n",
    "print(\"\\nMissingness (gp_clean):\\n\", gp.isna().mean().sort_values(ascending=False).head(8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6de152c",
   "metadata": {},
   "source": [
    "# ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a018a396",
   "metadata": {},
   "source": [
    "### Our Project Problems:\n",
    "    1) Concern: orbital safety risks from satellite crowding, drag anomalies, and decay events.\n",
    "    2) Need: detect, quantify, and predict risks from TLE + decay data. \n",
    "        \n",
    "### What the congestion index + anomaly analysis does?\n",
    "    1) Orbital Activity\n",
    "        Daily semi-major axis, perigee/apogee, mean motion → shows orbital drift.\n",
    "        Altitude bins = snapshots of where satellites are concentrated.\n",
    "\n",
    "    2) Anomalies\n",
    "        Outliers in perigee/apogee drift or sudden BSTAR spikes.\n",
    "        These can signal drag events, storms, or operational maneuvers.\n",
    "\n",
    "    3) Risks\n",
    "        Congestion index = satellites per altitude shell.\n",
    "        Identifies crowded “collision-risk” regions (like 500–600 km, where Starlink/LEO cubesats cluster).\n",
    "        Decay logs cross-checked → show whether drag events caused clustered re-entries.\n",
    "    \n",
    "### Relevance of this Project:\n",
    "    1) It quantifies orbital congestion → directly tied to sustainability and collision risk.\n",
    "    2) It detects anomalies in orbital elements → early warning for drag-driven failures.\n",
    "    3) It links to reentry/decay events → measurable outcomes of increased risk.\n",
    "    \n",
    "\n",
    "### Stepwise Flow We Now Have\n",
    "    1) Data ingestion: SATCAT, TLE, decay/reentry.\n",
    "    2) Cleaning: one snapshot/day per satellite.\n",
    "    3) Feature extraction: altitude, BSTAR, mean motion, anomalies.\n",
    "    4) Congestion index: binning satellites by altitude shell.\n",
    "    5) Risk indicators: overlay anomalies with decay clusters.\n",
    "    6) Insights: highlight “hot shells” & high-risk periods → exactly what your stakeholders (operators, regulators, insurers) care about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00160919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read rows: 10,175  Unique (NORAD, day): 1,816\n",
      "✔ Wrote gp_basic_clean.csv\n",
      "✔ Wrote congestion.csv\n",
      "✔ Wrote congestion_top_bins.csv\n",
      "\n",
      "Top 10 most crowded altitude shells (by avg satellites/day):\n",
      " 1.  400-425  km  →  1.0 sats/day\n"
     ]
    }
   ],
   "source": [
    "# === Pure-Python congestion index from Space-Track TLE CSV ===\n",
    "# - No numpy/pandas/matplotlib required.\n",
    "# - Input: your TLE/GP CSV export (e.g., \"st_....csv\")\n",
    "# - Outputs:\n",
    "#     1) gp_basic_clean.csv  -> one row per (NORAD, day) with computed altitudes\n",
    "#     2) congestion.csv      -> (date, bin_low_km, bin_high_km, count)\n",
    "#     3) congestion_top_bins.csv -> average count per bin over time (sorted)\n",
    "\n",
    "import csv, math, os\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# ---- CONFIG ----\n",
    "TLE_CSV = \"st_krutiray95@gmail_com_20250831_1445368442.csv\"  # <-- change if your filename differs\n",
    "OUT_GP_BASIC = \"gp_basic_clean.csv\"\n",
    "OUT_CONGESTION = \"congestion.csv\"\n",
    "OUT_CONGESTION_TOP = \"congestion_top_bins.csv\"\n",
    "\n",
    "# Altitude-bin config (LEO focus)\n",
    "BIN_START_KM = 150\n",
    "BIN_END_KM   = 2000\n",
    "BIN_STEP_KM  = 25\n",
    "\n",
    "# Earth/grav constants\n",
    "MU_KM3_S2 = 398600.4418\n",
    "EARTH_RADIUS_KM = 6378.137\n",
    "\n",
    "# ---- Helpers ----\n",
    "def get_first(row, keys, default=None):\n",
    "    \"\"\"Return first found value from a list of possible column names.\"\"\"\n",
    "    for k in keys:\n",
    "        if k in row and row[k] not in (None, \"\"):\n",
    "            return row[k]\n",
    "    return default\n",
    "\n",
    "def safe_float(s):\n",
    "    try:\n",
    "        return float(str(s).strip())\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def parse_epoch(s):\n",
    "    \"\"\"Robust ISO-ish parser handling 'Z' and ' UTC'.\"\"\"\n",
    "    if not s:\n",
    "        return None\n",
    "    t = s.strip()\n",
    "    if t.endswith(\"Z\"):\n",
    "        t = t[:-1]\n",
    "    if t.upper().endswith(\" UTC\"):\n",
    "        t = t[:-4]\n",
    "    # Replace 'T' with ' ' if present; tolerate fractional seconds\n",
    "    t = t.replace(\"T\", \" \")\n",
    "    try:\n",
    "        # Try common formats\n",
    "        for fmt in (\"%Y-%m-%d %H:%M:%S.%f\", \"%Y-%m-%d %H:%M:%S\", \"%Y-%m-%d\"):\n",
    "            try:\n",
    "                return datetime.strptime(t, fmt).replace(tzinfo=timezone.utc)\n",
    "            except ValueError:\n",
    "                pass\n",
    "        return None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def compute_altitudes(mean_motion_rev_per_day, ecc):\n",
    "    \"\"\"\n",
    "    From mean motion (rev/day) and eccentricity:\n",
    "      n(rad/s) = (MM * 2π) / 86400\n",
    "      a = (μ / n^2)^(1/3)\n",
    "      rp = a*(1-e), ra = a*(1+e)\n",
    "      hp = rp - Re, ha = ra - Re\n",
    "    Returns (a_km, hp_km, ha_km) or (None,None,None) if inputs invalid.\n",
    "    \"\"\"\n",
    "    if mean_motion_rev_per_day is None or mean_motion_rev_per_day <= 0:\n",
    "        return (None, None, None)\n",
    "    if ecc is None or not (0 <= ecc < 1):\n",
    "        return (None, None, None)\n",
    "    n = mean_motion_rev_per_day * 2.0 * math.pi / 86400.0\n",
    "    try:\n",
    "        a_km = (MU_KM3_S2 / (n*n)) ** (1.0/3.0)\n",
    "    except Exception:\n",
    "        return (None, None, None)\n",
    "    rp_km = a_km * (1.0 - ecc)\n",
    "    ra_km = a_km * (1.0 + ecc)\n",
    "    hp_km = rp_km - EARTH_RADIUS_KM\n",
    "    ha_km = ra_km - EARTH_RADIUS_KM\n",
    "    return (a_km, hp_km, ha_km)\n",
    "\n",
    "def make_bins(start_km, end_km, step_km):\n",
    "    bins = []\n",
    "    low = start_km\n",
    "    while low < end_km:\n",
    "        bins.append((low, min(low + step_km, end_km)))\n",
    "        low += step_km\n",
    "    return bins\n",
    "\n",
    "def bin_index_for(hp_km, bins):\n",
    "    \"\"\"Return index of the bin whose (low, high] contains hp_km, else None.\"\"\"\n",
    "    if hp_km is None:\n",
    "        return None\n",
    "    for i, (lo, hi) in enumerate(bins):\n",
    "        if hp_km > lo and hp_km <= hi:\n",
    "            return i\n",
    "    return None\n",
    "\n",
    "# ---- 1) Read TLE CSV and build per-(NORAD, day) latest snapshot ----\n",
    "if not os.path.exists(TLE_CSV):\n",
    "    raise FileNotFoundError(f\"Input TLE CSV not found: {TLE_CSV}\")\n",
    "\n",
    "per_day = {}  # key = (norad, date) -> dict with fields incl. hp_km, ha_km, etc., picks latest epoch\n",
    "\n",
    "with open(TLE_CSV, \"r\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    rows = 0\n",
    "    for row in reader:\n",
    "        rows += 1\n",
    "        # Handle varied column names from Space-Track exports\n",
    "        norad = get_first(row, [\"NORAD_CAT_ID\", \"NORAD\", \"NORADId\"])\n",
    "        mm    = get_first(row, [\"MEAN_MOTION\", \"MEANMOTION\", \"MeanMotion\"])\n",
    "        ecc   = get_first(row, [\"ECCENTRICITY\", \"ECC\"])\n",
    "        bstar = get_first(row, [\"BSTAR\", \"BSTARDRAG\", \"Bstar\"])\n",
    "        epoch = get_first(row, [\"EPOCH\", \"EPOCHUTC\", \"Epoch\"])\n",
    "\n",
    "        norad = int(float(norad)) if norad not in (None, \"\") else None\n",
    "        mm    = safe_float(mm)\n",
    "        ecc   = safe_float(ecc)\n",
    "        bstar = safe_float(bstar)\n",
    "        epoch_dt = parse_epoch(epoch)\n",
    "\n",
    "        if norad is None or epoch_dt is None or mm is None or ecc is None:\n",
    "            continue  # skip incomplete rows\n",
    "\n",
    "        a_km, hp_km, ha_km = compute_altitudes(mm, max(0.0, min(ecc, 0.999999)))\n",
    "\n",
    "        day = epoch_dt.date()\n",
    "        key = (norad, day)\n",
    "        # keep the LATEST epoch that day\n",
    "        prev = per_day.get(key)\n",
    "        if (prev is None) or (epoch_dt > prev[\"epoch_dt\"]):\n",
    "            per_day[key] = {\n",
    "                \"norad\": norad,\n",
    "                \"date\": day.isoformat(),\n",
    "                \"epoch_dt\": epoch_dt,\n",
    "                \"mean_motion\": mm,\n",
    "                \"eccentricity\": ecc,\n",
    "                \"bstar\": bstar,\n",
    "                \"a_km\": a_km,\n",
    "                \"hp_km\": hp_km,\n",
    "                \"ha_km\": ha_km,\n",
    "            }\n",
    "\n",
    "print(f\"Read rows: {rows:,}  Unique (NORAD, day): {len(per_day):,}\")\n",
    "\n",
    "# ---- 2) Write a lightweight cleaned file (one row per NORAD/day) ----\n",
    "with open(OUT_GP_BASIC, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow([\"NORAD_CAT_ID\",\"DATE\",\"MEAN_MOTION\",\"ECCENTRICITY\",\"BSTAR\",\"A_KM\",\"HP_KM\",\"HA_KM\"])\n",
    "    for (_, _), rec in sorted(per_day.items(), key=lambda kv: (kv[0][0], kv[0][1])):\n",
    "        w.writerow([\n",
    "            rec[\"norad\"], rec[\"date\"], \n",
    "            rec[\"mean_motion\"], rec[\"eccentricity\"], rec[\"bstar\"],\n",
    "            rec[\"a_km\"], rec[\"hp_km\"], rec[\"ha_km\"]\n",
    "        ])\n",
    "print(f\"✔ Wrote {OUT_GP_BASIC}\")\n",
    "\n",
    "# ---- 3) Build altitude bins & congestion index ----\n",
    "bins = make_bins(BIN_START_KM, BIN_END_KM, BIN_STEP_KM)\n",
    "\n",
    "# Count unique satellites per (date, bin)\n",
    "# We'll deduplicate by NORAD within each (date, bin)\n",
    "congestion_counts = {}         # key: (date, bin_idx) -> count\n",
    "seen_sets = {}                 # key: (date, bin_idx) -> set of NORADs (to avoid double count)\n",
    "\n",
    "for (norad, day), rec in per_day.items():\n",
    "    hp_km = rec[\"hp_km\"]\n",
    "    idx = bin_index_for(hp_km, bins)\n",
    "    if idx is None:\n",
    "        continue\n",
    "    key = (rec[\"date\"], idx)\n",
    "    if key not in seen_sets:\n",
    "        seen_sets[key] = set()\n",
    "    if norad not in seen_sets[key]:\n",
    "        seen_sets[key].add(norad)\n",
    "        congestion_counts[key] = congestion_counts.get(key, 0) + 1\n",
    "\n",
    "# Write congestion by day/bin\n",
    "with open(OUT_CONGESTION, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow([\"DATE\",\"BIN_LOW_KM\",\"BIN_HIGH_KM\",\"COUNT\"])\n",
    "    for (date_str, idx), count in sorted(congestion_counts.items(), key=lambda kv: (kv[0][0], kv[0][1])):\n",
    "        lo, hi = bins[idx]\n",
    "        w.writerow([date_str, lo, hi, count])\n",
    "print(f\"✔ Wrote {OUT_CONGESTION}\")\n",
    "\n",
    "# ---- 4) Average congestion per bin across time (to find 'crowded shells') ----\n",
    "# Sum & n-days per bin\n",
    "bin_sum = {}\n",
    "bin_days = {}\n",
    "for (date_str, idx), count in congestion_counts.items():\n",
    "    bin_sum[idx] = bin_sum.get(idx, 0) + count\n",
    "    bin_days[idx] = bin_days.get(idx, 0) + 1\n",
    "\n",
    "avg_per_bin = []\n",
    "for idx in sorted(bin_sum.keys()):\n",
    "    lo, hi = bins[idx]\n",
    "    avg = bin_sum[idx] / max(1, bin_days[idx])\n",
    "    avg_per_bin.append((lo, hi, avg))\n",
    "\n",
    "# Sort by average count descending and write top file\n",
    "avg_per_bin.sort(key=lambda t: t[2], reverse=True)\n",
    "\n",
    "with open(OUT_CONGESTION_TOP, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow([\"BIN_LOW_KM\",\"BIN_HIGH_KM\",\"AVG_COUNT_PER_DAY\"])\n",
    "    for lo, hi, avg in avg_per_bin:\n",
    "        w.writerow([lo, hi, f\"{avg:.2f}\"])\n",
    "print(f\"✔ Wrote {OUT_CONGESTION_TOP}\")\n",
    "\n",
    "# ---- 5) Print a small summary to the notebook output ----\n",
    "print(\"\\nTop 10 most crowded altitude shells (by avg satellites/day):\")\n",
    "for i, (lo, hi, avg) in enumerate(avg_per_bin[:10], start=1):\n",
    "    print(f\"{i:>2}. {lo:>4.0f}-{hi:<4.0f} km  →  {avg:.1f} sats/day\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d350a792",
   "metadata": {},
   "source": [
    "### CASE STUDY : Feb 2022 Starlink Storm\n",
    "    Layer Interpretation\n",
    "    (e.g., how the 2022 geomagnetic storm caused dozens of Starlinks to decay), supported by the congestion/anomaly data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0277d464",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the Window & Keywords\n",
    "#Basically focussing on 02-03-2022 to 02-15-2022 and the objects whose name contains 'STARLINK'.\n",
    "CASE_START = \"2022-02-03\"\n",
    "CASE_END   = \"2022-02-15\"\n",
    "KEYWORDS   = [\"STARLINK\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ad47152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9390"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Identifying the affected objects\n",
    "# 1A. From satcat.csv : Starlink NORADs in that period\n",
    "import csv, datetime as dt\n",
    "\n",
    "def in_window(d, start, end):\n",
    "    return start <= d <= end\n",
    "\n",
    "def parse_date(s):\n",
    "    for fmt in (\"%Y-%m-%d\", \"%Y-%m-%d %H:%M:%S\"):\n",
    "        try: return dt.datetime.strptime(s.strip(), fmt).date()\n",
    "        except: pass\n",
    "    return None\n",
    "\n",
    "starlink_meta = {}   # NORAD -> dict(name, country, launch, decay)\n",
    "start = dt.date.fromisoformat(CASE_START)\n",
    "end   = dt.date.fromisoformat(CASE_END)\n",
    "\n",
    "with open(\"satcat.csv\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    r = csv.DictReader(f)\n",
    "    for row in r:\n",
    "        name = (row.get(\"SATNAME\") or row.get(\"OBJECT_NAME\") or \"\").upper()\n",
    "        if not any(k in name for k in KEYWORDS):\n",
    "            continue\n",
    "        norad = row.get(\"NORAD_CAT_ID\") or row.get(\"OBJECT_NUMBER\")\n",
    "        if not norad: \n",
    "            continue\n",
    "        norad = int(float(norad))\n",
    "        launch = parse_date(row.get(\"LAUNCH\") or \"\")\n",
    "        # keep all starlinks; we’ll cross-filter by case window later\n",
    "        starlink_meta[norad] = {\n",
    "            \"name\": name,\n",
    "            \"country\": row.get(\"COUNTRY\"),\n",
    "            \"launch\": launch,\n",
    "            \"decay\": parse_date(row.get(\"DECAY\") or \"\")\n",
    "        }\n",
    "\n",
    "len(starlink_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94e25e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starlink decays in window: 80\n"
     ]
    }
   ],
   "source": [
    "# 1B. From decay.csv : who re-entered into the window?\n",
    "def parse_dt(s):\n",
    "    s = (s or \"\").strip()\n",
    "    s = s.replace(\"T\",\" \").replace(\"Z\",\"\")\n",
    "    for fmt in (\"%Y-%m-%d %H:%M:%S\", \"%Y-%m-%d %H:%M\", \"%Y-%m-%d\"):\n",
    "        try: \n",
    "            return dt.datetime.strptime(s, fmt)\n",
    "        except: pass\n",
    "    return None\n",
    "\n",
    "decayed = []  # list of dicts per decay occurrence in window\n",
    "\n",
    "with open(\"decay.csv\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    r = csv.DictReader(f)\n",
    "    for row in r:\n",
    "        name = (row.get(\"OBJECT_NAME\") or row.get(\"SATNAME\") or \"\").upper()\n",
    "        if not any(k in name for k in KEYWORDS): \n",
    "            continue\n",
    "        norad_s = row.get(\"NORAD_CAT_ID\") or row.get(\"OBJECT_NUMBER\")\n",
    "        if not norad_s: \n",
    "            continue\n",
    "        norad = int(float(norad_s))\n",
    "        dte = parse_dt(row.get(\"DECAY_EPOCH\"))\n",
    "        if not dte: \n",
    "            continue\n",
    "        if in_window(dte.date(), start, end):\n",
    "            decayed.append({\n",
    "                \"norad\": norad,\n",
    "                \"name\": name,\n",
    "                \"decay_epoch\": dte\n",
    "            })\n",
    "\n",
    "print(\"Starlink decays in window:\", len(decayed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e20efa3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohort medians (pre vs storm):\n",
      "HP_KM  : None → None (drop = None )\n",
      "MM     : None → None\n",
      "BSTAR  : None → None\n"
     ]
    }
   ],
   "source": [
    "#Showing the before/after drag signature for the affected cohort\n",
    "#Computing median perigee altitude (HP_KM), mean motion, and BSTAR for the cohort 7 days before vs during the storm.\n",
    "#Observations:\n",
    "#    HP_KM drops (perigee loss = increased drag).\n",
    "#    MEAN_MOTION rises a bit (shorter period as objects drop).\n",
    "#    BSTAR spikes (classic atmospheric-drag proxy).\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load daily GP features (your pure-Python output)\n",
    "def parse_float(x):\n",
    "    try: return float(x)\n",
    "    except: return None\n",
    "\n",
    "daily = []  # rows: dict(NORAD, DATE, HP_KM, MEAN_MOTION, BSTAR)\n",
    "with open(\"gp_basic_clean.csv\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    r = csv.DictReader(f)\n",
    "    for row in r:\n",
    "        try:\n",
    "            daily.append({\n",
    "                \"NORAD\": int(float(row[\"NORAD_CAT_ID\"])),\n",
    "                \"DATE\":  dt.date.fromisoformat(row[\"DATE\"]),\n",
    "                \"HP_KM\": parse_float(row[\"HP_KM\"]),\n",
    "                \"MM\":    parse_float(row[\"MEAN_MOTION\"]),\n",
    "                \"BSTAR\": parse_float(row[\"BSTAR\"]),\n",
    "            })\n",
    "        except: \n",
    "            pass\n",
    "\n",
    "decayed_ids = {d[\"norad\"] for d in decayed}\n",
    "\n",
    "# helper to median without numpy\n",
    "def median(vals):\n",
    "    vals = sorted(v for v in vals if v is not None)\n",
    "    if not vals: return None\n",
    "    n = len(vals)\n",
    "    if n % 2: return vals[n//2]\n",
    "    return 0.5*(vals[n//2-1] + vals[n//2])\n",
    "\n",
    "# Buckets: pre-storm (−7 to −1 days) vs storm window (Feb 3–8)\n",
    "pre_start = dt.date.fromisoformat(\"2022-01-27\")\n",
    "pre_end   = dt.date.fromisoformat(\"2022-02-02\")\n",
    "storm_start = dt.date.fromisoformat(\"2022-02-03\")\n",
    "storm_end   = dt.date.fromisoformat(\"2022-02-08\")\n",
    "\n",
    "pre_vals   = defaultdict(lambda: {\"hp\":[], \"mm\":[], \"bs\":[]})\n",
    "storm_vals = defaultdict(lambda: {\"hp\":[], \"mm\":[], \"bs\":[]})\n",
    "\n",
    "for row in daily:\n",
    "    if row[\"NORAD\"] not in decayed_ids: \n",
    "        continue\n",
    "    d = row[\"DATE\"]\n",
    "    if pre_start <= d <= pre_end:\n",
    "        pre_vals[row[\"NORAD\"]][\"hp\"].append(row[\"HP_KM\"])\n",
    "        pre_vals[row[\"NORAD\"]][\"mm\"].append(row[\"MM\"])\n",
    "        pre_vals[row[\"NORAD\"]][\"bs\"].append(row[\"BSTAR\"])\n",
    "    if storm_start <= d <= storm_end:\n",
    "        storm_vals[row[\"NORAD\"]][\"hp\"].append(row[\"HP_KM\"])\n",
    "        storm_vals[row[\"NORAD\"]][\"mm\"].append(row[\"MM\"])\n",
    "        storm_vals[row[\"NORAD\"]][\"bs\"].append(row[\"BSTAR\"])\n",
    "\n",
    "# Summarize cohort-level medians\n",
    "pre_hp    = median([median(v[\"hp\"]) for v in pre_vals.values()])\n",
    "storm_hp  = median([median(v[\"hp\"]) for v in storm_vals.values()])\n",
    "pre_mm    = median([median(v[\"mm\"]) for v in pre_vals.values()])\n",
    "storm_mm  = median([median(v[\"mm\"]) for v in storm_vals.values()])\n",
    "pre_bs    = median([median(v[\"bs\"]) for v in pre_vals.values()])\n",
    "storm_bs  = median([median(v[\"bs\"]) for v in storm_vals.values()])\n",
    "\n",
    "print(\"Cohort medians (pre vs storm):\")\n",
    "print(\"HP_KM  :\", pre_hp, \"→\", storm_hp, \"(drop =\", None if (pre_hp is None or storm_hp is None) else storm_hp - pre_hp, \")\")\n",
    "print(\"MM     :\", pre_mm, \"→\", storm_mm)\n",
    "print(\"BSTAR  :\", pre_bs, \"→\", storm_bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e020f481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline (Jan 1–Feb 2): 0\n",
      "Storm window (Feb 3–8): 38\n",
      "Follow-up (Feb 9–15):   42\n",
      "Excess during storm:    38\n"
     ]
    }
   ],
   "source": [
    "#Quantifying \"excess reentries\" Vs a Baseline\n",
    "#How unusual was the reentry count during the storm window compared to prior 30 days for Starlink objects?\n",
    "\n",
    "# Count reentries of starlink objects in two windows\n",
    "base_start = dt.date.fromisoformat(\"2022-01-01\")\n",
    "base_end   = dt.date.fromisoformat(\"2022-02-02\")\n",
    "\n",
    "def count_decays(window_start, window_end):\n",
    "    return sum(1 for d in decayed if window_start <= d[\"decay_epoch\"].date() <= window_end)\n",
    "\n",
    "baseline = count_decays(base_start, base_end)\n",
    "storm    = count_decays(storm_start, storm_end)\n",
    "followup = count_decays(storm_end + dt.timedelta(days=1), dt.date.fromisoformat(\"2022-02-15\"))\n",
    "\n",
    "print(f\"Baseline (Jan 1–Feb 2): {baseline}\")\n",
    "print(f\"Storm window (Feb 3–8): {storm}\")\n",
    "print(f\"Follow-up (Feb 9–15):   {followup}\")\n",
    "print(f\"Excess during storm:    {storm - baseline}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce588534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top storm-window crowded shells (150–500 km):\n",
      " 400- 425 km  →  1.0 sats/day\n"
     ]
    }
   ],
   "source": [
    "#Showing whether the affected cohort lived in a crowded shell (e.g., ~200–350 km injection or ~300–400 km raise phase). We’ll compute average congestion in those shells during the storm window.\n",
    "\n",
    "# Read congestion.csv (DATE, BIN_LOW_KM, BIN_HIGH_KM, COUNT)\n",
    "cong = []\n",
    "with open(\"congestion.csv\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    r = csv.DictReader(f)\n",
    "    for row in r:\n",
    "        try:\n",
    "            d = dt.date.fromisoformat(row[\"DATE\"])\n",
    "            lo = float(row[\"BIN_LOW_KM\"]); hi = float(row[\"BIN_HIGH_KM\"])\n",
    "            cnt = int(float(row[\"COUNT\"]))\n",
    "            if storm_start <= d <= storm_end and 150 <= lo <= 500:\n",
    "                cong.append(((lo,hi), cnt))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# Avg congestion by bin\n",
    "from collections import defaultdict\n",
    "sum_cnt, n_cnt = defaultdict(int), defaultdict(int)\n",
    "for (lo,hi), c in cong:\n",
    "    sum_cnt[(lo,hi)] += c\n",
    "    n_cnt[(lo,hi)]   += 1\n",
    "\n",
    "avg_by_bin = [ (lo,hi, sum_cnt[(lo,hi)]/max(1,n_cnt[(lo,hi)])) for (lo,hi) in sum_cnt ]\n",
    "avg_by_bin.sort(key=lambda t: t[2], reverse=True)\n",
    "\n",
    "print(\"Top storm-window crowded shells (150–500 km):\")\n",
    "for lo,hi,avg in avg_by_bin[:10]:\n",
    "    print(f\"{int(lo):4d}-{int(hi):4d} km  →  {avg:.1f} sats/day\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef75f751",
   "metadata": {},
   "source": [
    "### Explaining the Above Case Study:\n",
    "    1) Event & driver. On Feb 3–4, 2022, a CME-driven geomagnetic storm (Kp~5) increased thermospheric density; SpaceX later reported losing ~38–40 newly deployed Starlink satellites to atmospheric drag.\n",
    "    2) Observed signals in our data. The affected cohort shows perigee loss, mean-motion uptick, and BSTAR spikes during Feb 3–8 compared with the prior week—consistent with drag physics and published analyses.\n",
    "    3) Risk framing. During the storm window, the low-altitude shells also had high congestion, compounding risk (reentry/collision management), reinforcing why space-weather-aware operations and altitude-shell traffic management belong in SSA best practices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb6767b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
